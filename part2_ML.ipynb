{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMoBZDwjBAeWKDf6JeC+2WT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/peterjsadowski/Tutorial-Microbiome/blob/main/part2_ML.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Machine Learning for Microbiome Data Analysis\n",
        "\n",
        "This notebook presents a tutorial on using machine learning with sklearn for analyzing microbiome amplicon data.\n",
        "\n",
        "Author: Peter Sadowski\n",
        "\n",
        "Date: Feb 19 2023\n"
      ],
      "metadata": {
        "id": "VLVD62Nnmmud"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading Data\n",
        "\n",
        "For this example we use samples taken by UH Professor Anthony Amend's BOT662 class. Instructions:\n",
        "1. Download file from [this link](https://drive.google.com/drive/u/0/folders/1ueHCzkohAkXOW-ZcbqDgeLr8PnQ5Rxyd) (Google login required).\n",
        "1. Upload the zip file 'Class_Data_for_Phyloseq.zip' to the runtime.\n",
        "1. Unzip it below."
      ],
      "metadata": {
        "id": "ytgkH944nBEe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip Class_Data_for_Phyloseq.zip  # Assumes file is saved locally."
      ],
      "metadata": {
        "id": "dQVAS-Q1nGvm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sklearn\n",
        "import matplotlib.pylab as plt\n",
        "\n",
        "# Load data into pandas dataframe.\n",
        "df = pd.read_csv('OTUs.100.rep.count_table.csv')\n",
        "df = df.rename(columns={'Unnamed: 0': 'OTU'})\n",
        "df = df.set_index('OTU')\n",
        "\n",
        "meta_data = pd.read_csv('brom_meta.csv')\n",
        "meta_data = meta_data.set_index('sample_name')"
      ],
      "metadata": {
        "id": "CGg-I-yBnOPH"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Supervised machine learning\n",
        "\n",
        "In supervised learning, the goal is to predict some target variable _y_ from some input variable _x_. In a probabilistic machine learning model, the predictions will be in the form of a conditional probability distribution p(y|x), where this might be a discrete probability mass function over a finite set of class labels (classification) or a probability density function over real values (regression). Classification is more concrete, so we focus on it here.\n"
      ],
      "metadata": {
        "id": "04VVh1CjnQbv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Logistic Regression Classifier\n",
        "Below we use a simple __logistic regression__ classifier to predict the sample_type from amplicon counts. \n",
        "\n"
      ],
      "metadata": {
        "id": "zA1fsiQow2nW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "def assign_colors(df, meta_data, colorby='sample_type'):\n",
        "  \"\"\" Assign a integer label and color to each unique value in the colorby col.\n",
        "  Returns:\n",
        "    labels = List of labels. Has length equal to len(df.columns).\n",
        "    labels_int = List of integer labels.\n",
        "    colors = List of colors. Has length equal to len(df.columns).\n",
        "    label_colors = Dict mapping unique keys to colors.\n",
        "  \"\"\"\n",
        "  labels = [meta_data.loc[[sample_name]][colorby][0]\n",
        "          for sample_name in df.columns]\n",
        "  encoder = LabelEncoder()\n",
        "  labels_int = encoder.fit_transform(labels)\n",
        "  cmap = plt.get_cmap('Accent')\n",
        "  num_labels = len(np.unique(labels))\n",
        "  label_colors = {label: cmap(i/num_labels) \n",
        "                  for i, label in enumerate(np.unique(labels))}\n",
        "  colors = [label_colors[label] for label in labels]\n",
        "  return labels, labels_int, colors, label_colors\n",
        "\n",
        "labels, labels_int, colors, label_colors = assign_colors(\n",
        "    df, meta_data, colorby='sample_type')\n",
        "\n",
        "# Classification task is to predict sample_type from \n",
        "X = (np.log(1+df).T).to_numpy()\n",
        "y = labels_int \n",
        "\n",
        "clf = LogisticRegression(random_state=0, penalty='l2', max_iter=1000).fit(X, y)\n",
        "# This function predicts the most likely class label.\n",
        "labels_predicted = clf.predict(X[:, :])\n",
        "# This function gives probabilities for each class.\n",
        "probs_predicted = clf.predict_proba(X[:, :])\n",
        "\n",
        "print(f'Accuracy: {clf.score(X, y) * 100}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-hZ2X3GqoH3i",
        "outputId": "9ebff5d0-1f7a-40ba-bf12-335144339946"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 100.0%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cross-Validation\n",
        "This model gets 100% accuracy on our little dataset of 22 examples. Is that good? We _don't know_ without testing it on new data! Machine learning models, even simple ones like this linear model, are prone to __overfit__ to the data they are trained on. A powerful model can _always_ get 100% on the training data, but that doesn't mean the model __generalizes__ to new examples (which is the only thing we ever care about). Thus, the standard way to evaluate ML models is through __cross-validation__ where the data is separated into a __training set__ that is used to fit the model and __test set__ that is only used for evaluating the model performance.\n",
        "\n",
        "Scikit learn has some very helpful methods managing these data splits. For example, train_test_split method splits off 40% of the data to be in the test set.\n"
      ],
      "metadata": {
        "id": "PnkJxqR5uvsG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4)\n",
        "clf = LogisticRegression(penalty='l2', max_iter=1000).fit(X_train, y_train)\n",
        "print(f'Accuracy: {clf.score(X_test, y_test) * 100}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eoEEi4hOwWAm",
        "outputId": "33e43836-01e1-46fd-9218-3b1d7e6cc522"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 33.33%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is far less than the 100% accuracy from before. This shouldn't be surprising, since we are only training on ~13 examples! Isn't there a way to use more of our tiny dataset for training? Unfortunatley, if we decrease the size of our test set, we get a very noisy estimate of the performance. Indeed, if the test set contains only one example, then the accuracy estimate will be either 0% or 100%.\n",
        "\n",
        "There is a nice trick that gives us the best of both worlds, at the cost of more computation. In __K-fold cross-validation__, we split the data into k subsets, and train k models, using a different subset for testing each iteration. In the extreme case where k is the number of samples in the dataset, this is called __leave-one-out cross validation__.  "
      ],
      "metadata": {
        "id": "TPgVX_P8x7Gf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import metrics\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "clf = LogisticRegression(random_state=0, penalty='l2', max_iter=1000)\n",
        "%timeit scores = cross_val_score(clf, X, y, cv=5)\n",
        "print(f'CV accuracies: {scores}')\n",
        "print(f'Mean accuracy: {scores.mean():.2f}')  # prints only two decimals"
      ],
      "metadata": {
        "id": "AWGfrj1gyqd-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "By default, Scikit Learn attempts to do __stratified cross-validation__, in which the examples in each class are evenly distributed between the k-folds. \n",
        "\n",
        "In order to do leave-one out cross-validation, we can pass the LeaveOneOut iterator object to cross_val_score. Notice that this takes a minute to run, because it is training 22 different models. "
      ],
      "metadata": {
        "id": "OXM58dTM1AsK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import LeaveOneOut\n",
        "clf = LogisticRegression(random_state=0, penalty='l2', max_iter=1000)\n",
        "%timeit scores = cross_val_score(clf, X, y, cv=LeaveOneOut())\n",
        "print(f'CV accuracies: {scores}')\n",
        "print(f'Mean accuracy: {scores.mean():.2f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P1npHfx50zgn",
        "outputId": "14ed663f-0e21-4ed3-de97-ac696165311a"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "25 s ± 1.96 s per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
            "CV accuracies: [0.6  0.8  1.   1.   0.75]\n",
            "Mean accuracy: 0.8300000000000001\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Decision Trees\n",
        "\n",
        "One of the most intuitive and interpretable machine learning algorithms is the decision tree. Training a decision tree consists of iteratively splitting the data space and assigning labels to each section.\n",
        "\n",
        "![decision boundary](https://scikit-learn.org/stable/_images/sphx_glr_plot_iris_dtc_001.png)\n",
        "\n"
      ],
      "metadata": {
        "id": "li95u-zvwnPo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Suppress those warnings about having a class with only one example.\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "\n",
        "clf = DecisionTreeClassifier(max_depth=3)\n",
        "%timeit scores = cross_val_score(clf, X, y, cv=5)\n",
        "print(f'CV accuracies: {scores}')\n",
        "print(f'Mean accuracy: {scores.mean():.2f}')"
      ],
      "metadata": {
        "id": "_uw2HtiZ36Te"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-Yt50Je34zcF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}